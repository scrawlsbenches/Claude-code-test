# CRITICAL GitHub Issues 6-10 - Remediation Sprint Phase 1

**Created:** 2025-11-20
**Sprint:** Phase 1 (Weeks 1-3)
**Milestone:** Phase 1 - Critical Remediation

---

## Issue #6: [CRITICAL] Race Condition in LoadBalanced Routing Strategy

**Priority:** üî¥ CRITICAL
**Category:** Concurrency / Thread Safety
**Assigned To:** Infrastructure Track Lead
**Estimated Effort:** 6 hours
**Phase:** 1 - Week 2

### Problem Description

`LoadBalancedRoutingStrategy` has integer overflow risk and race condition. After 2.1 billion calls, `_currentIndex` overflows to negative, causing `IndexOutOfRangeException`. Additionally, subscription count can change between lock acquisitions.

### Impact

**SERVICE CRASH AT SCALE**
- `IndexOutOfRangeException` crashes message routing
- Occurs after ~24 days at 1000 msg/sec
- Race condition if subscriptions added/removed during routing
- Incorrect load distribution
- Message delivery failures

### Code Location

**File:** `src/HotSwap.Distributed.Orchestrator/Routing/LoadBalancedRoutingStrategy.cs`

**Lines:** 14, 64-71

**Current Code:**
```csharp
// Line 14 - No overflow protection
private int _currentIndex = 0;

// Lines 64-71 - Race condition
lock (_lock)
{
    selectedIndex = _currentIndex % activeSubscriptions.Count; // ‚ùå Can overflow
    selectedSubscription = activeSubscriptions[selectedIndex];

    // Increment for next call
    _currentIndex = (_currentIndex + 1) % activeSubscriptions.Count; // ‚ùå Overflow risk
}
```

### Recommended Fix

**Option A: Add Overflow Protection**

```csharp
private int _currentIndex = 0;
private readonly object _lock = new object();

public async Task<RouteResult> RouteAsync(Message message, IReadOnlyList<Subscription> subscriptions, ...)
{
    var activeSubscriptions = subscriptions.Where(s => s.IsActive).ToList();

    if (activeSubscriptions.Count == 0)
        return RouteResult.CreateFailure("LoadBalanced", "No active consumers");

    Subscription selectedSubscription;
    int selectedIndex;

    lock (_lock)
    {
        // Prevent overflow - reset when approaching max
        if (_currentIndex >= int.MaxValue - 1000 || _currentIndex < 0)
        {
            _currentIndex = 0;
            _logger.LogInformation("Round-robin index reset due to overflow protection");
        }

        selectedIndex = _currentIndex % activeSubscriptions.Count;
        selectedSubscription = activeSubscriptions[selectedIndex];
        _currentIndex++;
    }

    return RouteResult.CreateSuccess("LoadBalanced",
        new[] { selectedSubscription.SubscriptionId },
        $"Round-robin selected consumer {selectedIndex} of {activeSubscriptions.Count}",
        new Dictionary<string, object>
        {
            ["totalActive"] = activeSubscriptions.Count,
            ["selectedIndex"] = selectedIndex
        });
}
```

**Option B: Stateless Routing (Recommended for Distributed Systems)**

```csharp
// Use message ID hash for deterministic routing
public async Task<RouteResult> RouteAsync(Message message, IReadOnlyList<Subscription> subscriptions, ...)
{
    var activeSubscriptions = subscriptions.Where(s => s.IsActive).ToList();

    if (activeSubscriptions.Count == 0)
        return RouteResult.CreateFailure("LoadBalanced", "No active consumers");

    // Deterministic selection based on message ID (no state needed)
    var hash = message.MessageId.GetHashCode();
    var selectedIndex = Math.Abs(hash) % activeSubscriptions.Count;
    var selectedSubscription = activeSubscriptions[selectedIndex];

    return RouteResult.CreateSuccess("LoadBalanced",
        new[] { selectedSubscription.SubscriptionId },
        $"Hash-based selection: consumer {selectedIndex} of {activeSubscriptions.Count}",
        new Dictionary<string, object>
        {
            ["totalActive"] = activeSubscriptions.Count,
            ["selectedIndex"] = selectedIndex,
            ["messageHash"] = hash
        });
}
```

### Acceptance Criteria

- [ ] No integer overflow after 3 billion calls
- [ ] No `IndexOutOfRangeException` under any condition
- [ ] Thread-safe routing
- [ ] Load distribution remains even
- [ ] Concurrent routing test passes (1000 threads)
- [ ] Overflow test passes (int.MaxValue iterations)
- [ ] All existing tests pass

### Test Cases

**Test 1: No Overflow After Many Calls**
```csharp
[Fact]
public async Task RouteAsync_After3BillionCalls_DoesNotOverflow()
{
    // Arrange
    var strategy = new LoadBalancedRoutingStrategy();
    var subscriptions = CreateSubscriptions(count: 3);

    // Simulate index at near-max value
    typeof(LoadBalancedRoutingStrategy)
        .GetField("_currentIndex", BindingFlags.NonPublic | BindingFlags.Instance)
        .SetValue(strategy, int.MaxValue - 10);

    // Act - 20 more calls (should overflow without protection)
    for (int i = 0; i < 20; i++)
    {
        var result = await strategy.RouteAsync(CreateMessage($"msg-{i}"), subscriptions);
        result.Success.Should().BeTrue();
    }

    // Assert - No exception thrown
}
```

**Test 2: Concurrent Routing Safety**
```csharp
[Fact]
public async Task RouteAsync_With1000ConcurrentCalls_DistributesEvenly()
{
    // Arrange
    var strategy = new LoadBalancedRoutingStrategy();
    var subscriptions = CreateSubscriptions(count: 10);
    var consumerCounts = new ConcurrentDictionary<string, int>();

    // Act - 1000 concurrent calls
    var tasks = Enumerable.Range(0, 1000).Select(async i =>
    {
        var result = await strategy.RouteAsync(CreateMessage($"msg-{i}"), subscriptions);
        consumerCounts.AddOrUpdate(result.ConsumerIds[0], 1, (k, v) => v + 1);
    });

    await Task.WhenAll(tasks);

    // Assert - Each consumer gets ~100 messages (¬±10%)
    foreach (var count in consumerCounts.Values)
    {
        count.Should().BeInRange(90, 110);
    }
}
```

**Test 3: Subscription Changes Don't Cause Crash**
```csharp
[Fact]
public async Task RouteAsync_WhenSubscriptionsChange_DoesNotCrash()
{
    // Arrange
    var strategy = new LoadBalancedRoutingStrategy();
    var subscriptions = new List<Subscription>(CreateSubscriptions(count: 5));

    // Act - Route while modifying subscription list
    var routingTasks = Enumerable.Range(0, 100).Select(async i =>
    {
        return await strategy.RouteAsync(CreateMessage($"msg-{i}"), subscriptions);
    });

    var modificationTask = Task.Run(async () =>
    {
        await Task.Delay(10);
        subscriptions.RemoveAt(0); // Remove subscription mid-routing
        await Task.Delay(10);
        subscriptions.Add(CreateSubscription("sub-new"));
    });

    // Assert - No crashes
    var results = await Task.WhenAll(routingTasks.Append(modificationTask));
}
```

### Definition of Done

- [x] Overflow protection implemented
- [x] All 3 tests passing
- [x] Concurrency test passes (1000 threads)
- [x] Code review approved
- [x] Load distribution verified
- [x] Merged to main

### Related Issues

- See CODE_REVIEW_REPORT.md section: "Orchestrator Issues #9"
- Related to: Issue #8 (Pipeline state race condition)

---

## Issue #7: [CRITICAL] Division by Zero in Canary Metrics Analysis

**Priority:** üî¥ CRITICAL
**Category:** Stability / Error Handling
**Assigned To:** Application Track Lead
**Estimated Effort:** 4 hours
**Phase:** 1 - Week 2

### Problem Description

`CanaryDeploymentStrategy` performs division by baseline metrics without zero protection. If baseline CPU/memory/latency is 0, `DivideByZeroException` crashes the deployment.

### Impact

**DEPLOYMENT CRASHES**
- Production deployments fail mid-execution
- Rollback may be incomplete
- Service downtime
- Occurs with fresh nodes (no baseline metrics)

### Code Location

**File:** `src/HotSwap.Distributed.Orchestrator/Strategies/CanaryDeploymentStrategy.cs`

**Lines:** 204-207

**Current Code:**
```csharp
// Lines 204-207 - NO ZERO PROTECTION
var cpuIncrease = (avgCanaryMetrics.CpuUsage - baseline.AvgCpuUsage) / baseline.AvgCpuUsage * 100;
var memoryIncrease = (avgCanaryMetrics.MemoryUsage - baseline.AvgMemoryUsage) / baseline.AvgMemoryUsage * 100;
var latencyIncrease = (avgCanaryMetrics.Latency - baseline.AvgLatency) / baseline.AvgLatency * 100;
var errorRateIncrease = (avgCanaryMetrics.ErrorRate - baseline.AvgErrorRate) / Math.Max(baseline.AvgErrorRate, 0.1) * 100;
// Note: Line 207 uses Math.Max but wrong threshold
```

### Recommended Fix

Create safe division helper:

```csharp
private double CalculatePercentageIncrease(double current, double baseline, string metricName)
{
    // Handle zero or near-zero baseline
    if (baseline <= 0.001)
    {
        _logger.LogWarning(
            "Baseline {MetricName} is near-zero ({Baseline}). " +
            "Cannot calculate percentage increase. Current value: {Current}",
            metricName, baseline, current);

        // If current is also near-zero, no significant change
        if (current <= 0.001)
            return 0.0;

        // If current is significantly higher than zero baseline, treat as 100% increase
        return 100.0;
    }

    return ((current - baseline) / baseline) * 100.0;
}

// Use consistently
var cpuIncrease = CalculatePercentageIncrease(
    avgCanaryMetrics.CpuUsage, baseline.AvgCpuUsage, "CPU");
var memoryIncrease = CalculatePercentageIncrease(
    avgCanaryMetrics.MemoryUsage, baseline.AvgMemoryUsage, "Memory");
var latencyIncrease = CalculatePercentageIncrease(
    avgCanaryMetrics.Latency, baseline.AvgLatency, "Latency");
var errorRateIncrease = CalculatePercentageIncrease(
    avgCanaryMetrics.ErrorRate, baseline.AvgErrorRate, "ErrorRate");
```

### Acceptance Criteria

- [ ] No `DivideByZeroException` under any scenario
- [ ] Zero baseline handled gracefully
- [ ] Warning logged when baseline is zero
- [ ] Metrics comparison still works correctly
- [ ] Canary deployment succeeds with zero baseline
- [ ] All existing tests pass
- [ ] New edge case tests added

### Test Cases

**Test 1: Zero Baseline CPU**
```csharp
[Fact]
public async Task CanaryDeployment_WithZeroBaselineCpu_DoesNotCrash()
{
    // Arrange
    var baseline = new BaselineMetrics
    {
        AvgCpuUsage = 0.0, // ‚ùå Zero baseline
        AvgMemoryUsage = 100.0,
        AvgLatency = 50.0,
        AvgErrorRate = 0.01
    };

    var canaryMetrics = new CanaryMetrics
    {
        CpuUsage = 10.0, // Increase from zero
        MemoryUsage = 105.0,
        Latency = 52.0,
        ErrorRate = 0.01
    };

    // Act
    var action = async () => await AnalyzeCanaryMetrics(baseline, canaryMetrics);

    // Assert - No exception
    await action.Should().NotThrowAsync<DivideByZeroException>();
}
```

**Test 2: All Zero Baselines**
```csharp
[Fact]
public async Task CanaryDeployment_WithAllZeroBaselines_ReturnsValidResult()
{
    // Arrange - Fresh deployment, no baseline
    var baseline = new BaselineMetrics
    {
        AvgCpuUsage = 0.0,
        AvgMemoryUsage = 0.0,
        AvgLatency = 0.0,
        AvgErrorRate = 0.0
    };

    var canaryMetrics = new CanaryMetrics
    {
        CpuUsage = 10.0,
        MemoryUsage = 100.0,
        Latency = 50.0,
        ErrorRate = 0.01
    };

    // Act
    var result = await AnalyzeCanaryMetrics(baseline, canaryMetrics);

    // Assert
    result.Should().NotBeNull();
    result.IsHealthy.Should().BeTrue(); // First deployment, no comparison
}
```

**Test 3: Near-Zero Baselines**
```csharp
[Fact]
public async Task CanaryDeployment_WithNearZeroBaseline_HandlesGracefully()
{
    // Arrange
    var baseline = new BaselineMetrics
    {
        AvgCpuUsage = 0.0001, // Near zero
        AvgMemoryUsage = 100.0,
        AvgLatency = 50.0,
        AvgErrorRate = 0.01
    };

    var canaryMetrics = new CanaryMetrics
    {
        CpuUsage = 10.0,
        MemoryUsage = 105.0,
        Latency = 52.0,
        ErrorRate = 0.01
    };

    // Act & Assert - Should not crash or produce NaN
    var result = await AnalyzeCanaryMetrics(baseline, canaryMetrics);
    result.CpuIncreasePercent.Should().NotBe(double.NaN);
    result.CpuIncreasePercent.Should().NotBe(double.PositiveInfinity);
}
```

### Definition of Done

- [x] Safe division helper implemented
- [x] All 3 tests passing
- [x] No division by zero possible
- [x] Warning logging added
- [x] Code review approved
- [x] Merged to main

### Related Issues

- See CODE_REVIEW_REPORT.md section: "Orchestrator Issues #10"

---

## Issue #8: [CRITICAL] Pipeline State Management Race Condition

**Priority:** üî¥ CRITICAL
**Category:** Concurrency / State Management
**Assigned To:** Application Track Lead
**Estimated Effort:** 10 hours
**Phase:** 1 - Week 2

### Problem Description

`UpdatePipelineStateAsync` in `DeploymentPipeline` has no synchronization. Multiple concurrent pipeline stages can update state simultaneously, causing lost updates, incorrect progress, and out-of-order notifications.

### Impact

**INCONSISTENT DEPLOYMENT STATE**
- UI shows incorrect pipeline progress
- Monitoring dashboards unreliable
- Audit logs incomplete or out of order
- Approval workflow sees stale state
- Customer-facing status incorrect

### Code Location

**File:** `src/HotSwap.Distributed.Orchestrator/Pipeline/DeploymentPipeline.cs`

**Lines:** 782-836

**Current Code:**
```csharp
// Lines 782-836 - NO SYNCHRONIZATION
private async Task UpdatePipelineStateAsync(
    DeploymentRequest request,
    string status,
    string? currentStage,
    List<PipelineStageResult> stages)
{
    // ‚ùå No locking - concurrent updates can interfere
    var state = new PipelineExecutionState
    {
        ExecutionId = request.ExecutionId,
        Request = request,
        Status = status,
        CurrentStage = currentStage,
        Stages = stages.ToList(), // Shallow copy - still mutable
        StartTime = stages.FirstOrDefault()?.StartTime ?? DateTime.UtcNow,
        LastUpdated = DateTime.UtcNow
    };

    if (_deploymentTracker != null)
    {
        await _deploymentTracker.UpdatePipelineStateAsync(request.ExecutionId, state);
    }

    // SignalR notifications could be out of order
    await _notificationService?.NotifyProgressUpdateAsync(state);
}
```

### Recommended Fix

**Add per-execution synchronization:**

```csharp
private readonly ConcurrentDictionary<Guid, SemaphoreSlim> _executionLocks = new();

private async Task UpdatePipelineStateAsync(
    DeploymentRequest request,
    string status,
    string? currentStage,
    List<PipelineStageResult> stages)
{
    // Get or create lock for this execution ID
    var executionLock = _executionLocks.GetOrAdd(
        request.ExecutionId,
        _ => new SemaphoreSlim(1, 1));

    await executionLock.WaitAsync();
    try
    {
        // Create immutable state
        var state = new PipelineExecutionState
        {
            ExecutionId = request.ExecutionId,
            Request = request,
            Status = status,
            CurrentStage = currentStage,
            Stages = stages.Select(s => s.Clone()).ToList(), // Deep copy
            StartTime = stages.FirstOrDefault()?.StartTime ?? DateTime.UtcNow,
            LastUpdated = DateTime.UtcNow,
            SequenceNumber = GetNextSequenceNumber(request.ExecutionId) // Ordering
        };

        // Update tracker
        if (_deploymentTracker != null)
        {
            await _deploymentTracker.UpdatePipelineStateAsync(request.ExecutionId, state);
        }

        // Notify (notifications now ordered by sequence number)
        if (_notificationService != null)
        {
            await _notificationService.NotifyProgressUpdateAsync(state);
        }
    }
    finally
    {
        executionLock.Release();
    }
}

// Cleanup locks for completed deployments
private async Task CleanupCompletedDeploymentLocks(Guid executionId)
{
    if (_executionLocks.TryRemove(executionId, out var semaphore))
    {
        semaphore.Dispose();
    }
}
```

### Acceptance Criteria

- [ ] Per-execution locking implemented
- [ ] No lost updates under concurrent stage completion
- [ ] State transitions are ordered (sequence numbers)
- [ ] SignalR notifications arrive in order
- [ ] Lock cleanup for completed deployments
- [ ] Concurrency test passes (10 stages complete simultaneously)
- [ ] All existing tests pass

### Test Cases

**Test 1: Concurrent Stage Updates**
```csharp
[Fact]
public async Task UpdatePipelineState_With10ConcurrentStages_AllUpdatesPreserved()
{
    // Arrange
    var request = CreateDeploymentRequest();
    var stages = Enumerable.Range(1, 10)
        .Select(i => CreateStageResult($"Stage{i}", PipelineStageStatus.Running))
        .ToList();

    // Act - Update 10 stages concurrently
    var updateTasks = stages.Select(async stage =>
    {
        stage.Status = PipelineStageStatus.Succeeded;
        await _pipeline.UpdatePipelineStateAsync(request, "Running", stage.StageName, stages);
    });

    await Task.WhenAll(updateTasks);

    // Assert - All 10 stage updates recorded
    var finalState = await _deploymentTracker.GetPipelineStateAsync(request.ExecutionId);
    finalState.Stages.Count(s => s.Status == PipelineStageStatus.Succeeded).Should().Be(10);
}
```

**Test 2: Notification Ordering**
```csharp
[Fact]
public async Task UpdatePipelineState_NotificationsArrivedInOrder()
{
    // Arrange
    var receivedNotifications = new ConcurrentBag<PipelineExecutionState>();
    _mockNotificationService
        .Setup(x => x.NotifyProgressUpdateAsync(It.IsAny<PipelineExecutionState>()))
        .Callback<PipelineExecutionState>(state => receivedNotifications.Add(state))
        .Returns(Task.CompletedTask);

    var request = CreateDeploymentRequest();
    var stages = CreateStages(count: 5);

    // Act - Update stages concurrently
    var updateTasks = stages.Select(async (stage, index) =>
    {
        await Task.Delay(Random.Shared.Next(0, 50)); // Random timing
        await _pipeline.UpdatePipelineStateAsync(request, "Running", stage.StageName, stages);
    });

    await Task.WhenAll(updateTasks);

    // Assert - Notifications have increasing sequence numbers
    var notifications = receivedNotifications.OrderBy(n => n.LastUpdated).ToList();
    for (int i = 1; i < notifications.Count; i++)
    {
        notifications[i].SequenceNumber.Should().BeGreaterThan(notifications[i - 1].SequenceNumber);
    }
}
```

**Test 3: No Deadlocks**
```csharp
[Fact]
public async Task UpdatePipelineState_100ConcurrentUpdates_CompletesWithin5Seconds()
{
    // Arrange
    var request = CreateDeploymentRequest();
    var stages = CreateStages(count: 1);
    var stopwatch = Stopwatch.StartNew();

    // Act - 100 concurrent updates
    var updateTasks = Enumerable.Range(0, 100).Select(async i =>
    {
        await _pipeline.UpdatePipelineStateAsync(
            request, $"Status{i}", $"Stage{i}", stages);
    });

    await Task.WhenAll(updateTasks);
    stopwatch.Stop();

    // Assert - No deadlock, completes quickly
    stopwatch.Elapsed.Should().BeLessThan(TimeSpan.FromSeconds(5));
}
```

### Definition of Done

- [x] Locking implemented per execution ID
- [x] All 3 tests passing
- [x] Sequence numbers added for ordering
- [x] Lock cleanup implemented
- [x] Code review approved
- [x] Merged to main

### Related Issues

- See CODE_REVIEW_REPORT.md section: "Orchestrator Issues #11"
- Related to: Issue #6 (LoadBalanced race condition)

---

## Issue #9: [CRITICAL] IDOR Vulnerability - Missing Resource Ownership Checks

**Priority:** üî¥ CRITICAL
**Category:** Security - Authorization
**Assigned To:** Security Track Lead
**Estimated Effort:** 12 hours
**Phase:** 1 - Week 2-3

### Problem Description

Controllers validate authentication/authorization but **don't verify resource ownership**. Any authenticated user can access resources (deployments, messages, etc.) belonging to other users or tenants.

### Impact

**HORIZONTAL PRIVILEGE ESCALATION**
- User A can access User B's deployments
- Tenant 1 can access Tenant 2's data
- Complete breakdown of authorization
- Regulatory compliance violation
- Customer data breach risk

### Code Locations

**Multiple Controllers Affected:**

1. **DeploymentsController.cs:141**
```csharp
[Authorize(Roles = "Viewer,Deployer,Admin")]
public async Task<IActionResult> GetDeployment(Guid executionId)
{
    var result = await _deploymentTracker.GetResultAsync(executionId);
    // ‚ùå NO CHECK: Does this deployment belong to authenticated user's tenant?
    return Ok(result);
}
```

2. **MessagesController.cs:95**
```csharp
public async Task<IActionResult> GetMessage(string id, ...)
{
    var message = await _messagePersistence.RetrieveAsync(id, ...);
    // ‚ùå NO CHECK: Does this message belong to current tenant?
    return Ok(message);
}
```

3. **ApprovalsController.cs, WebsitesController.cs, etc.**

### Recommended Fix

**Create authorization helper:**

```csharp
// NEW FILE: Api/Authorization/ResourceAuthorizationHelper.cs
public class ResourceAuthorizationHelper
{
    private readonly ITenantContextService _tenantContext;
    private readonly IHttpContextAccessor _httpContext;

    public async Task<bool> CanAccessDeploymentAsync(Guid executionId)
    {
        var currentTenantId = await _tenantContext.GetCurrentTenantIdAsync();
        var deployment = await _deploymentTracker.GetResultAsync(executionId);

        if (deployment == null)
            return false;

        // Check tenant ownership
        if (deployment.TenantId != currentTenantId)
        {
            _logger.LogWarning(
                "IDOR attempt: User from tenant {CurrentTenant} tried to access " +
                "deployment {ExecutionId} belonging to tenant {OwnerTenant}",
                currentTenantId, executionId, deployment.TenantId);
            return false;
        }

        // Optionally: Check user-level ownership for non-admins
        if (!User.IsInRole("Admin") && deployment.RequesterEmail != User.Identity.Name)
        {
            return false;
        }

        return true;
    }
}
```

**Update controller:**

```csharp
[Authorize(Roles = "Viewer,Deployer,Admin")]
public async Task<IActionResult> GetDeployment(Guid executionId)
{
    // ‚úÖ CHECK OWNERSHIP
    if (!await _authHelper.CanAccessDeploymentAsync(executionId))
    {
        return Forbid(); // 403 Forbidden
    }

    var result = await _deploymentTracker.GetResultAsync(executionId);
    return Ok(result);
}
```

### Acceptance Criteria

- [ ] Authorization helper created
- [ ] All GET endpoints verify ownership (Deployments, Messages, Approvals, Websites)
- [ ] All POST/PUT/DELETE endpoints verify ownership
- [ ] Cross-tenant access returns 403 Forbidden
- [ ] IDOR attempts logged with warning
- [ ] Same-tenant access works normally
- [ ] Admin users can access all resources (optional)
- [ ] All existing tests pass
- [ ] New IDOR security tests added

### Test Cases

**Test 1: Cross-Tenant Deployment Access**
```csharp
[Fact]
public async Task GetDeployment_FromDifferentTenant_Returns403()
{
    // Arrange
    var tenant1Deployment = await CreateDeployment(tenantId: "tenant1");
    var tenant2Client = CreateAuthenticatedClient(tenantId: "tenant2");

    // Act - Tenant 2 tries to access Tenant 1's deployment
    var response = await tenant2Client.GetAsync(
        $"/api/v1/deployments/{tenant1Deployment.ExecutionId}");

    // Assert
    response.StatusCode.Should().Be(HttpStatusCode.Forbidden);
}
```

**Test 2: Same-Tenant Access Works**
```csharp
[Fact]
public async Task GetDeployment_FromSameTenant_Returns200()
{
    // Arrange
    var deployment = await CreateDeployment(tenantId: "tenant1");
    var client = CreateAuthenticatedClient(tenantId: "tenant1");

    // Act
    var response = await client.GetAsync(
        $"/api/v1/deployments/{deployment.ExecutionId}");

    // Assert
    response.StatusCode.Should().Be(HttpStatusCode.OK);
    var result = await response.Content.ReadAsAsync<DeploymentResult>();
    result.ExecutionId.Should().Be(deployment.ExecutionId);
}
```

**Test 3: User-Level Ownership (Non-Admin)**
```csharp
[Fact]
public async Task GetDeployment_ByDifferentUser_Returns403ForNonAdmin()
{
    // Arrange
    var deployment = await CreateDeployment(
        tenantId: "tenant1",
        requesterEmail: "user1@tenant1.com");

    var client = CreateAuthenticatedClient(
        tenantId: "tenant1",
        userEmail: "user2@tenant1.com",
        roles: "Viewer"); // Not admin

    // Act
    var response = await client.GetAsync(
        $"/api/v1/deployments/{deployment.ExecutionId}");

    // Assert
    response.StatusCode.Should().Be(HttpStatusCode.Forbidden);
}
```

**Test 4: Admin Can Access All Resources**
```csharp
[Fact]
public async Task GetDeployment_ByAdmin_Returns200ForAnyUser()
{
    // Arrange
    var deployment = await CreateDeployment(
        tenantId: "tenant1",
        requesterEmail: "user1@tenant1.com");

    var client = CreateAuthenticatedClient(
        tenantId: "tenant1",
        userEmail: "admin@tenant1.com",
        roles: "Admin");

    // Act
    var response = await client.GetAsync(
        $"/api/v1/deployments/{deployment.ExecutionId}");

    // Assert
    response.StatusCode.Should().Be(HttpStatusCode.OK);
}
```

**Test 5-8:** Repeat for Messages, Approvals, Websites controllers

### Definition of Done

- [x] Authorization helper implemented
- [x] All 8+ controllers updated
- [x] All 8 tests passing (2 per controller type)
- [x] Security review approved
- [x] IDOR attempts logged
- [x] Code review approved
- [x] Merged to main

### Related Issues

- See CODE_REVIEW_REPORT.md section: "API Security Issues #16"
- Related to: Issue #2 (Tenant isolation), Issue #1 (Schema auth)

---

## Issue #10: [CRITICAL] Unchecked Rollback Failures

**Priority:** üî¥ CRITICAL
**Category:** Error Handling / Stability
**Assigned To:** Infrastructure Track Lead
**Estimated Effort:** 10 hours
**Phase:** 1 - Week 3

### Problem Description

Deployment strategies perform rollbacks when deployment fails, but rollback failures are logged without corrective action. System left in mixed state (some nodes old version, some new version).

### Impact

**INCONSISTENT PRODUCTION STATE**
- Mixed deployment versions across nodes
- Manual intervention required
- Difficult to debug which nodes are in which state
- Service degradation
- No operator notification

### Code Locations

**Multiple Strategy Files:**

1. **RollingDeploymentStrategy.cs:147-179**
2. **CanaryDeploymentStrategy.cs:247-266**

**Current Code:**
```csharp
private async Task RollbackAllAsync(...)
{
    var rollbackTasks = successfulDeployments.Select(async nodeResult =>
    {
        var node = cluster.GetNode(nodeResult.NodeId);
        if (node != null)
        {
            return await node.RollbackModuleAsync(moduleName);
        }
        return new NodeRollbackResult { Success = false, Message = "Node not found" };
    });

    var rollbackResults = await Task.WhenAll(rollbackTasks);
    result.RollbackResults.AddRange(rollbackResults);

    var rollbackSuccessCount = rollbackResults.Count(r => r.Success);
    result.RollbackSuccessful = rollbackSuccessCount == rollbackResults.Length;

    // ‚ùå NO ACTION taken if rollback fails!
    // Just logs and returns
}
```

### Recommended Fix

**Add failure handling and alerting:**

```csharp
private async Task RollbackAllAsync(
    EnvironmentCluster cluster,
    List<NodeDeploymentResult> successfulDeployments,
    string moduleName,
    DeploymentResult result,
    CancellationToken cancellationToken)
{
    var rollbackTasks = successfulDeployments.Select(async nodeResult =>
    {
        var node = cluster.GetNode(nodeResult.NodeId);
        if (node == null)
        {
            return new NodeRollbackResult
            {
                NodeId = nodeResult.NodeId,
                Success = false,
                Message = "Node not found in cluster"
            };
        }

        // Retry rollback with exponential backoff
        return await RetryRollbackAsync(node, moduleName, maxRetries: 3);
    });

    var rollbackResults = await Task.WhenAll(rollbackTasks);
    result.RollbackResults.AddRange(rollbackResults);

    var rollbackSuccessCount = rollbackResults.Count(r => r.Success);
    result.RollbackSuccessful = rollbackSuccessCount == rollbackResults.Length;

    // ‚úÖ HANDLE ROLLBACK FAILURES
    if (!result.RollbackSuccessful)
    {
        var failedNodes = rollbackResults.Where(r => !r.Success).ToList();

        // Log CRITICAL error
        _logger.LogCritical(
            "‚ö†Ô∏è ROLLBACK FAILED for {FailedCount}/{TotalCount} nodes in {Environment}. " +
            "Module: {ModuleName}. Failed Nodes: {FailedNodeIds}. " +
            "MANUAL INTERVENTION REQUIRED!",
            failedNodes.Count,
            rollbackResults.Length,
            cluster.EnvironmentType,
            moduleName,
            string.Join(", ", failedNodes.Select(r => r.NodeId)));

        // Send critical alert to operators
        await _alertService?.SendCriticalAlertAsync(new CriticalAlert
        {
            Severity = AlertSeverity.Critical,
            Title = $"Rollback Failure - {cluster.EnvironmentType}",
            Description = $"Failed to rollback {moduleName} on {failedNodes.Count} nodes",
            AffectedNodes = failedNodes.Select(r => r.NodeId).ToList(),
            RecommendedAction = "Manually inspect nodes and restore to known good state",
            IncidentId = result.DeploymentId
        }, cancellationToken);

        // Mark nodes as "needs manual intervention"
        foreach (var failedRollback in failedNodes)
        {
            await _nodeRegistry?.MarkNodeAsRequiringManualIntervention(
                failedRollback.NodeId,
                $"Rollback failed for {moduleName}: {failedRollback.Message}");
        }

        // Optionally: Take nodes out of service
        if (_config.AutoQuarantineFailedRollbacks)
        {
            foreach (var failedRollback in failedNodes)
            {
                await cluster.QuarantineNode(failedRollback.NodeId,
                    $"Automatic quarantine due to rollback failure");
            }
        }
    }
    else
    {
        _logger.LogInformation(
            "Rollback successful for all {Count} nodes in {Environment}",
            rollbackResults.Length, cluster.EnvironmentType);
    }
}

private async Task<NodeRollbackResult> RetryRollbackAsync(
    KernelNode node,
    string moduleName,
    int maxRetries)
{
    for (int attempt = 1; attempt <= maxRetries; attempt++)
    {
        try
        {
            var result = await node.RollbackModuleAsync(moduleName);
            if (result.Success)
                return result;

            _logger.LogWarning(
                "Rollback attempt {Attempt}/{MaxRetries} failed for node {NodeId}: {Message}",
                attempt, maxRetries, node.Id, result.Message);

            if (attempt < maxRetries)
                await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, attempt))); // Exponential backoff
        }
        catch (Exception ex)
        {
            _logger.LogError(ex,
                "Rollback attempt {Attempt}/{MaxRetries} threw exception for node {NodeId}",
                attempt, maxRetries, node.Id);
        }
    }

    return new NodeRollbackResult
    {
        NodeId = node.Id,
        Success = false,
        Message = $"Rollback failed after {maxRetries} attempts"
    };
}
```

### Acceptance Criteria

- [ ] Rollback failures trigger critical alerts
- [ ] Failed nodes marked for manual intervention
- [ ] Retry logic with exponential backoff (3 attempts)
- [ ] Optional auto-quarantine of failed nodes
- [ ] Critical logging at appropriate level
- [ ] All existing tests pass
- [ ] New rollback failure tests added

### Test Cases

**Test 1: Partial Rollback Failure Triggers Alert**
```csharp
[Fact]
public async Task RollbackAll_WhenSomeNodesFail_SendsCriticalAlert()
{
    // Arrange
    var cluster = CreateCluster(nodeCount: 5);
    var successfulDeployments = CreateDeploymentResults(count: 5);

    // Simulate: 2 nodes rollback successfully, 3 fail
    _mockNodes[0].SetupRollbackSuccess();
    _mockNodes[1].SetupRollbackSuccess();
    _mockNodes[2].SetupRollbackFailure("Network timeout");
    _mockNodes[3].SetupRollbackFailure("Module not found");
    _mockNodes[4].SetupRollbackFailure("Disk full");

    // Act
    var result = new DeploymentResult();
    await _strategy.RollbackAllAsync(cluster, successfulDeployments, "TestModule", result);

    // Assert
    result.RollbackSuccessful.Should().BeFalse();
    _mockAlertService.Verify(
        x => x.SendCriticalAlertAsync(
            It.Is<CriticalAlert>(alert =>
                alert.Severity == AlertSeverity.Critical &&
                alert.AffectedNodes.Count == 3),
            It.IsAny<CancellationToken>()),
        Times.Once);
}
```

**Test 2: Rollback Retries With Exponential Backoff**
```csharp
[Fact]
public async Task RetryRollback_Retries3TimesWithBackoff()
{
    // Arrange
    var node = CreateMockNode();
    var callTimes = new List<DateTime>();

    node.Setup(x => x.RollbackModuleAsync(It.IsAny<string>()))
        .Callback(() => callTimes.Add(DateTime.UtcNow))
        .ReturnsAsync(new NodeRollbackResult { Success = false });

    // Act
    await RetryRollbackAsync(node.Object, "TestModule", maxRetries: 3);

    // Assert - 3 attempts made
    callTimes.Should().HaveCount(3);

    // Assert - Exponential backoff (2s, 4s between attempts)
    var delay1 = (callTimes[1] - callTimes[0]).TotalSeconds;
    var delay2 = (callTimes[2] - callTimes[1]).TotalSeconds;

    delay1.Should().BeInRange(1.9, 2.5); // ~2 seconds
    delay2.Should().BeInRange(3.9, 4.5); // ~4 seconds
}
```

**Test 3: Successful Rollback Doesn't Alert**
```csharp
[Fact]
public async Task RollbackAll_WhenAllSucceed_DoesNotSendAlert()
{
    // Arrange
    var cluster = CreateCluster(nodeCount: 5);
    var successfulDeployments = CreateDeploymentResults(count: 5);

    // All nodes rollback successfully
    foreach (var mockNode in _mockNodes)
    {
        mockNode.SetupRollbackSuccess();
    }

    // Act
    var result = new DeploymentResult();
    await _strategy.RollbackAllAsync(cluster, successfulDeployments, "TestModule", result);

    // Assert
    result.RollbackSuccessful.Should().BeTrue();
    _mockAlertService.Verify(
        x => x.SendCriticalAlertAsync(It.IsAny<CriticalAlert>(), It.IsAny<CancellationToken>()),
        Times.Never);
}
```

### Definition of Done

- [x] Retry logic implemented (3 attempts, exponential backoff)
- [x] Critical alerting implemented
- [x] Node marking for manual intervention
- [x] All 3 tests passing
- [x] Logging at CRITICAL level
- [x] Code review approved
- [x] Merged to main

### Related Issues

- See CODE_REVIEW_REPORT.md section: "Orchestrator Issues #12"

---

**(10 of 15 issues created. Continuing with final 5...)**
