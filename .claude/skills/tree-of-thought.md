# Tree-of-Thought Problem-Solving Framework

**Description**: Systematic problem-solving framework for complex investigations, debugging, and multi-step implementations. Prevents analysis paralysis, circular reasoning, and getting stuck while maintaining execution bias.

**When to use**:
- âœ… Complex investigations (expected >30 minutes)
- âœ… Stuck/deadlocked after 3-5 attempts on a problem
- âœ… Unfamiliar problem domain requiring systematic exploration
- âœ… Multi-step feature implementation with many unknowns
- âœ… User explicitly requests thorough/systematic approach
- âŒ **DON'T USE** for simple bugs (just fix it)
- âŒ **DON'T USE** for straightforward features (TDD Red-Green-Blue is enough)
- âŒ **DON'T USE** for code reviews (just review)
- âŒ **DON'T USE** for quick investigations (<15 minutes)

**Core Philosophy**: ğŸ¯ **80/20 RULE** - You only need Phase -1 + TDD for 80% of tasks. This framework is for the complex 20%.

---

## Instructions

**Purpose**: This guide helps you work systematically on the Claude-code-test distributed hot-swap orchestration project while avoiding common pitfalls like analysis paralysis, circular reasoning, and getting stuck.

**âš¡ MOST IMPORTANT**: Bias toward ACTION. When in doubt, write code. You can course-correct faster by doing than by overthinking.

---

## Table of Contents

- [âš¡ Phase -1: Execution Bias](#-phase--1-execution-bias-read-this-first) â­â­â­ **START HERE**
- [ğŸ¯ Quick Reference: 80% of Tasks](#-quick-reference-80-of-tasks) â­â­â­
- [ğŸ“‹ Full Tree-of-Thought Framework](#-full-tree-of-thought-framework) â­
- [ğŸ”„ Iteration & Context Management](#-iteration--context-management) â­
- [ğŸ”“ Deadlock Detection & Escape Sequences](#-deadlock-detection--escape-sequences) â­
- [ğŸ§  Advanced Topics](#-advanced-topics)

---

## âš¡ Phase -1: Execution Bias (READ THIS FIRST!)

**BIAS TOWARD ACTION: When in doubt, execute. Code is cheap, overthinking is expensive.**

```
EXECUTION BIAS RULES
â”‚
â”œâ”€ Rule 1: START FAST
â”‚  â””â”€ If task is clear (>80% confidence) â†’ START IMMEDIATELY
â”‚     â””â”€ Don't plan, don't read docs, just write the test (TDD)
â”‚     â””â”€ You can course-correct faster by doing than by thinking
â”‚
â”œâ”€ Rule 2: PHASE TIME LIMITS (HARD LIMITS)
â”‚  â”œâ”€ Phase 0 (Doc Skepticism): 30 seconds - "Is doc obviously wrong?"
â”‚  â”œâ”€ Phase 1 (Understanding): 1 minute - "Do I get what's being asked?"
â”‚  â”œâ”€ Phase 2 (Environment): 30 seconds - "Do I have .NET SDK?"
â”‚  â”œâ”€ Phase 3 (Context): 2 minutes - "What code do I need to read?"
â”‚  â”œâ”€ Phase 4 (Planning): 2 minutes - "Red/Green/Blue, what's the test?"
â”‚  â””â”€ Phase 5+ (Execution): UNLIMITED - Actually doing the work
â”‚
â”‚  ğŸš¨ If you exceed time limits â†’ STOP PLANNING, START DOING
â”‚
â”œâ”€ Rule 3: "WHEN IN DOUBT" DEFAULTS
â”‚  â”œâ”€ Unclear requirement? â†’ Write simplest test you CAN write, refine later
â”‚  â”œâ”€ Multiple approaches? â†’ Pick the simplest, don't deliberate
â”‚  â”œâ”€ Missing info? â†’ Make reasonable assumption, note it, proceed
â”‚  â”œâ”€ Doc vs code conflict? â†’ Trust code, fix doc in same commit
â”‚  â””â”€ Complex task? â†’ Start with smallest piece, expand from there
â”‚
â”œâ”€ Rule 4: STOP SIGNALS (Execute immediately when you catch yourself)
â”‚  â”œâ”€ "Let me think about this more..." â†’ NO, write a test NOW
â”‚  â”œâ”€ "I should check one more thing..." â†’ NO, check WHILE coding
â”‚  â”œâ”€ "Maybe I should read..." â†’ NO, read the MINIMUM, start coding
â”‚  â”œâ”€ "What if X or Y or Z..." â†’ NO, pick X, try it, pivot if wrong
â”‚  â””â”€ Reading this framework for >2 minutes â†’ STOP, go write code
â”‚
â””â”€ Rule 5: DONE IS BETTER THAN PERFECT
   â”œâ”€ Write the imperfect test â†’ Makes progress
   â”œâ”€ Write the naive implementation â†’ Can refactor in BLUE phase
   â”œâ”€ Ship the minimal feature â†’ Can enhance in next iteration
   â””â”€ Make the "good enough" commit â†’ Can improve in next PR
```

**â±ï¸ EXECUTION TRIGGER: If you've been thinking >5 minutes without writing code/files â†’ You're stuck in planning. Execute NOW.**

**ğŸ¯ 80/20 RULE: You only need Phase -1 + TDD Red-Green-Blue for 80% of tasks. The rest is for complex debugging only.**

---

## ğŸ¯ Quick Reference: 80% of Tasks

**For straightforward tasks (new feature, simple bug fix, refactoring):**

```
FAST PATH (5 minutes from request to first test)
â”‚
1. âš¡ UNDERSTAND (30 seconds)
   â””â”€ What exactly am I building/fixing?
   â””â”€ If clear â†’ Proceed. If unclear â†’ Ask user ONE clarifying question.

2. ğŸ”§ ENVIRONMENT (30 seconds)
   â””â”€ Do I have .NET SDK? (dotnet --version)
      â”œâ”€ YES â†’ Can build/test locally
      â””â”€ NO â†’ Use alternative checklist (verify contracts carefully)

3. ğŸ“š CONTEXT (2 minutes MAX)
   â””â”€ What existing code/tests relate to this?
   â””â”€ Use Grep/Glob to find relevant files
   â””â”€ Read ONLY what's directly relevant (not the whole codebase!)

4. ğŸ”´ RED - Write Failing Test (5-10 minutes)
   â””â”€ tests/HotSwap.Distributed.Tests/[Component]Tests.cs
   â””â”€ Test name: MethodName_StateUnderTest_ExpectedBehavior
   â””â”€ AAA pattern: Arrange â†’ Act â†’ Assert (FluentAssertions)
   â””â”€ Run test: dotnet test --filter "TestName"
   â””â”€ MUST FAIL (if passes, test is wrong!)

5. ğŸŸ¢ GREEN - Minimal Implementation (10-20 minutes)
   â””â”€ Write ONLY enough code to make test pass
   â””â”€ Don't worry about perfection yet
   â””â”€ Run test: MUST PASS

6. ğŸ”µ BLUE - Refactor for Quality (5-10 minutes)
   â””â”€ Improve naming, extract methods, add docs
   â””â”€ Run ALL tests: dotnet test (must still pass!)

7. âœ… VERIFY & COMMIT (5 minutes)
   â””â”€ dotnet clean && dotnet restore && dotnet build --no-incremental
   â””â”€ dotnet test (all tests pass?)
   â””â”€ Update docs if needed (CLAUDE.md, README.md)
   â””â”€ git add . && git commit -m "feat: ..."
   â””â”€ git pull origin <branch> --no-rebase (handle conflicts if any)
   â””â”€ git push -u origin claude/<branch-name>

TOTAL TIME: ~30-40 minutes for typical feature
```

**If you're taking longer than this â†’ You're overthinking. Simplify or ask for help.**

---

## ğŸ“‹ Full Tree-of-Thought Framework

**Use this for complex investigations, debugging, or unfamiliar areas:**

### ğŸ¯ Phase 0: Documentation Skepticism

**âš ï¸ CRITICAL ASSUMPTION: Documentation can be outdated, incorrect, or misleading.**

**Philosophy: "Trust but Verify" - Code is the source of truth, documentation is a guide.**

```
DOCUMENTATION SKEPTICISM (30 second check)
â”‚
â”œâ”€ When reading ANY documentation, ask:
â”‚  â”œâ”€ When was this last updated? (Check "Last Updated" date)
â”‚  â”œâ”€ Does this match what I see in the code?
â”‚  â””â”€ If mismatch â†’ Code is right, doc is wrong
â”‚
â”œâ”€ Sources of truth (in priority order):
â”‚  1. **The actual code** (.cs files) â†’ ALWAYS AUTHORITATIVE
â”‚  2. **Test files** â†’ Show actual usage and behavior
â”‚  3. **Build/test output** â†’ Shows current state
â”‚  4. **Type definitions** â†’ Compiler-verified contracts
â”‚  5. **Git history** â†’ Recent changes not yet documented
â”‚  6. **Documentation** â†’ Helpful guide, but verify critical info
â”‚
â”œâ”€ When I find a documentation error, classify severity:
â”‚  â”‚
â”‚  â”œâ”€ ğŸ”´ BLOCKER (Fix immediately, blocks current work)
â”‚  â”‚  â”œâ”€ Doc says class/method exists, but it doesn't
â”‚  â”‚  â”œâ”€ Doc says to use API that's been removed
â”‚  â”‚  â”œâ”€ Setup instructions don't work at all
â”‚  â”‚  â””â”€ Action: MUST resolve NOW to proceed
â”‚  â”‚
â”‚  â”œâ”€ ğŸŸ  CRITICAL (Could cause bugs/security issues)
â”‚  â”‚  â”œâ”€ Doc shows insecure code example
â”‚  â”‚  â”œâ”€ Doc misleads about error handling
â”‚  â”‚  â”œâ”€ Doc contradicts actual method signatures
â”‚  â”‚  â””â”€ Action: Fix in current commit if <10 min, else high-priority task
â”‚  â”‚
â”‚  â”œâ”€ ğŸŸ¡ MAJOR (Outdated but not immediately dangerous)
â”‚  â”‚  â”œâ”€ Test counts are wrong (doc: 65, actual: 582)
â”‚  â”‚  â”œâ”€ Package versions are outdated
â”‚  â”‚  â”œâ”€ File references point to old structure
â”‚  â”‚  â””â”€ Action: Fix if in same area I'm modifying, else create task
â”‚  â”‚
â”‚  â””â”€ ğŸŸ¢ MINOR (Cosmetic, low impact)
â”‚     â”œâ”€ Typos, formatting issues
â”‚     â”œâ”€ Slightly outdated wording
â”‚     â””â”€ Action: Batch with other doc updates, or ignore if trivial
â”‚
â””â”€ Ground Truth Verification:
   â”œâ”€ For Type definitions: Use Glob + Read actual .cs file
   â”œâ”€ For Method signatures: Use Grep for method definition
   â”œâ”€ For Test counts: Run dotnet test --verbosity quiet
   â”œâ”€ For Package versions: Run dotnet list package
   â””â”€ For Project structure: Run ls -R src/ tests/
```

**Decision: Fix doc error now or defer?**

```
Found doc error â†’ Does it BLOCK my current task?
â”‚
â”œâ”€ YES (BLOCKER) â†’ Fix immediately, proceed with correct info
â”œâ”€ Could cause bugs? (CRITICAL) â†’ Fix if <10 min, else create task
â”œâ”€ Already modifying this area? (MAJOR) â†’ Fix in current commit
â””â”€ Minor/cosmetic? (MINOR) â†’ Add TODO comment, defer to monthly cleanup
```

---

### ğŸ¯ Phase 1: Understanding & Scoping

**(1 minute time limit - if taking longer, ask user for clarification)**

```
1. REQUEST CLARITY
   â”œâ”€ What exactly is being requested?
   â”‚  â”œâ”€ New feature? â†’ Identify acceptance criteria
   â”‚  â”œâ”€ Bug fix? â†’ Can I reproduce it? What's expected behavior?
   â”‚  â”œâ”€ Refactoring? â†’ What's the goal?
   â”‚  â””â”€ Documentation? â†’ What changed that triggered this?
   â”‚
   â”œâ”€ Is the request clear and unambiguous?
   â”‚  â”œâ”€ YES â†’ Proceed to Phase 2
   â”‚  â””â”€ NO â†’ Ask ONE specific clarifying question, don't guess
   â”‚
   â””â”€ What's the scope?
      â”œâ”€ Single file/component? â†’ Straightforward
      â”œâ”€ Multiple components? â†’ Break into smaller tasks, use TodoWrite
      â””â”€ Cross-cutting concern? â†’ Check Domain/Infrastructure/Orchestrator/API layers
```

**â±ï¸ Time limit exceeded? â†’ Make your best assumption, note it, proceed. You can pivot later if wrong.**

---

### ğŸ¯ Phase 2: Environment & Prerequisites

**(30 second check)**

```
2. ENVIRONMENT CHECK
   â”œâ”€ Do I have .NET SDK 8.0+ installed?
   â”‚  â”œâ”€ YES â†’ I can build and test locally (use standard checklist)
   â”‚  â”‚  â””â”€ Verify: dotnet --version (Expected: 8.0.121+)
   â”‚  â”‚
   â”‚  â””â”€ NO â†’ I'm in a restricted environment (use alternative checklist)
   â”‚     â”œâ”€ MUST verify contracts extra carefully (read all type definitions)
   â”‚     â”œâ”€ MUST manually check package references
   â”‚     â””â”€ MUST rely on CI/CD for verification
   â”‚
   â””â”€ Is there a relevant Claude Skill I should use?
      â”œâ”€ /tdd-helper â†’ For test-driven development workflow
      â”œâ”€ /precommit-check â†’ For pre-commit validation
      â”œâ”€ /test-coverage-analyzer â†’ For coverage analysis
      â””â”€ See SKILLS.md for full list
```

---

### ğŸ¯ Phase 3: Context Gathering

**(2 minute time limit - gather MINIMUM needed context)**

```
3. CODE CONTEXT (Read ONLY what's directly relevant)
   â”‚
   â”œâ”€ What contracts (interfaces, models, enums) will I use?
   â”‚  â”œâ”€ Use Grep to find: grep -r "interface IUserService"
   â”‚  â”œâ”€ Use Read to examine: Read the actual .cs file
   â”‚  â”œâ”€ Check: Property names, method signatures, nullability
   â”‚  â””â”€ âš ï¸ NEVER GUESS property/method names - always verify!
   â”‚
   â”œâ”€ What patterns exist in this codebase?
   â”‚  â”œâ”€ Find similar code: Use Grep for similar functionality
   â”‚  â”œâ”€ Check test patterns: How are other services tested?
   â”‚  â””â”€ Follow existing conventions (don't reinvent)
   â”‚
   â””â”€ What layers are involved?
      â”œâ”€ Domain â†’ Core models, enums (no dependencies)
      â”œâ”€ Infrastructure â†’ Cross-cutting (telemetry, security)
      â”œâ”€ Orchestrator â†’ Core orchestration logic
      â””â”€ API â†’ REST controllers (depends on all above)
```

**â±ï¸ Time limit exceeded? â†’ You're reading too much. Start with what you have, read more WHILE coding if needed.**

---

### ğŸ¯ Phase 4: Planning & Task Management

**(2 minute time limit - simple plan only)**

```
4. WORK PLANNING
   â”‚
   â”œâ”€ Is this complex enough for TodoWrite?
   â”‚  â”œâ”€ YES (3+ steps) â†’ Create todo list:
   â”‚  â”‚  â”œâ”€ ğŸ”´ Write test for [feature]
   â”‚  â”‚  â”œâ”€ ğŸŸ¢ Implement [feature] to pass test
   â”‚  â”‚  â”œâ”€ ğŸ”µ Refactor [feature] implementation
   â”‚  â”‚  â””â”€ âœ… Verify all tests pass
   â”‚  â”‚
   â”‚  â””â”€ NO (1-2 simple steps) â†’ Just do it, skip TodoWrite
   â”‚
   â””â”€ What's my TDD strategy?
      â”œâ”€ New Feature â†’ Write tests first for expected behavior
      â”‚  â”œâ”€ Happy path test
      â”‚  â”œâ”€ Edge case tests (null, empty, boundary)
      â”‚  â””â”€ Error case tests (invalid input, exceptions)
      â”‚
      â”œâ”€ Bug Fix â†’ Write test that reproduces bug (should fail)
      â””â”€ Refactoring â†’ Ensure existing tests exist and pass first
```

**â±ï¸ Time limit exceeded? â†’ Stop planning. Write the first test NOW. Plan adjusts as you go.**

---

### ğŸ¯ Phase 5-7: Test-Driven Development (TDD)

**This is where you spend most of your time (actual work!):**

#### ğŸ”´ Phase 5: RED - Write Failing Test

```
WRITE FAILING TESTS (NO TIME LIMIT - this is actual work)
â”‚
â”œâ”€ Test file location:
â”‚  â””â”€ tests/HotSwap.Distributed.Tests/[ComponentName]Tests.cs
â”‚
â”œâ”€ Test naming convention:
â”‚  â””â”€ MethodName_StateUnderTest_ExpectedBehavior
â”‚     Example: AuthenticateAsync_WithValidCredentials_ReturnsToken
â”‚
â”œâ”€ Test structure (AAA pattern):
â”‚  â”œâ”€ // Arrange - Set up test data, mocks, system under test
â”‚  â”œâ”€ // Act - Execute the method being tested
â”‚  â””â”€ // Assert - Verify expected behavior (use FluentAssertions)
â”‚
â”œâ”€ Mock setup patterns:
â”‚  â”œâ”€ Read the ACTUAL interface/method signature (don't guess!)
â”‚  â”œâ”€ Mock ALL parameters exactly (including CancellationToken)
â”‚  â””â”€ Use It.IsAny<T>() for parameters you don't care about
â”‚
â”œâ”€ Run the test - it MUST FAIL:
â”‚  â”œâ”€ Command: dotnet test --filter "FullyQualifiedName~[TestName]"
â”‚  â”œâ”€ Expected: Test fails (implementation doesn't exist yet)
â”‚  â””â”€ If test passes without implementation â†’ TEST IS WRONG, fix it!
â”‚
â””â”€ Package references for tests:
   â”œâ”€ Does test use BCrypt? â†’ Test project needs BCrypt.Net-Next
   â”œâ”€ Does test use ILogger? â†’ Test project needs Microsoft.Extensions.Logging.Abstractions
   â””â”€ Check: tests/HotSwap.Distributed.Tests/HotSwap.Distributed.Tests.csproj
```

#### ğŸŸ¢ Phase 6: GREEN - Minimal Implementation

```
IMPLEMENT TO PASS (NO TIME LIMIT - this is actual work)
â”‚
â”œâ”€ Implementation location:
â”‚  â”œâ”€ Controllers â†’ src/HotSwap.Distributed.Api/Controllers/
â”‚  â”œâ”€ Services â†’ src/HotSwap.Distributed.Orchestrator/Services/
â”‚  â”œâ”€ Models â†’ src/HotSwap.Distributed.Domain/Models/
â”‚  â””â”€ Infrastructure â†’ src/HotSwap.Distributed.Infrastructure/
â”‚
â”œâ”€ Implementation checklist:
â”‚  â”œâ”€ Use EXACT property/method names from contracts (no guessing!)
â”‚  â”œâ”€ Follow namespace conventions (match folder structure)
â”‚  â”œâ”€ Add required using statements
â”‚  â”œâ”€ Use async/await for I/O operations
â”‚  â”œâ”€ Add proper error handling (try-catch where appropriate)
â”‚  â””â”€ Don't optimize yet - just make it work!
â”‚
â”œâ”€ Run the test - it MUST PASS:
â”‚  â”œâ”€ Command: dotnet test --filter "FullyQualifiedName~[TestName]"
â”‚  â”œâ”€ Expected: Test passes (implementation is correct)
â”‚  â””â”€ If test fails â†’ Fix implementation, NOT the test!
â”‚
â””â”€ Dependency injection (if needed):
   â”œâ”€ Register new services in Program.cs
   â”œâ”€ Use appropriate lifetime (Singleton/Scoped/Transient)
   â””â”€ Follow existing registration patterns
```

#### ğŸ”µ Phase 7: BLUE - Refactor for Quality

```
REFACTOR FOR QUALITY (NO TIME LIMIT - this is actual work)
â”‚
â”œâ”€ Code quality improvements:
â”‚  â”œâ”€ Extract methods for complex logic
â”‚  â”œâ”€ Improve variable/method naming
â”‚  â”œâ”€ Remove duplication (DRY principle)
â”‚  â”œâ”€ Apply SOLID principles
â”‚  â”œâ”€ Add XML documentation for public APIs
â”‚  â””â”€ Add inline comments for complex logic
â”‚
â”œâ”€ Run ALL tests continuously:
â”‚  â”œâ”€ Command: dotnet test
â”‚  â”œâ”€ Expected: ALL tests pass (including existing tests)
â”‚  â””â”€ If ANY test fails â†’ Revert and try smaller refactoring steps
â”‚
â”œâ”€ Security considerations:
â”‚  â”œâ”€ No hardcoded secrets or credentials
â”‚  â”œâ”€ Input validation for user input
â”‚  â”œâ”€ Parameterized queries (avoid SQL injection)
â”‚  â”œâ”€ Sanitize output (avoid XSS)
â”‚  â””â”€ Use using statements for IDisposable resources
â”‚
â””â”€ Performance considerations:
   â”œâ”€ Use async/await for I/O-bound operations
   â”œâ”€ Avoid blocking calls (no .Result or .Wait())
   â”œâ”€ Consider appropriate collection types
   â””â”€ Profile before optimizing (don't guess)
```

---

### ğŸ¯ Phase 8: Documentation Updates

```
DOCUMENTATION SYNC (5 minutes)
â”‚
â”œâ”€ What documentation needs updating?
â”‚  â”œâ”€ Changed public APIs? â†’ Update XML comments, README.md
â”‚  â”œâ”€ Added/removed packages? â†’ Update CLAUDE.md Technology Stack
â”‚  â”œâ”€ Changed build/test process? â†’ Update CLAUDE.md setup instructions
â”‚  â”œâ”€ Added/removed tests? â†’ Update test counts in CLAUDE.md
â”‚  â”œâ”€ Changed project structure? â†’ Update CLAUDE.md Project Structure
â”‚  â”œâ”€ Completed TASK_LIST.md task? â†’ Update status, add to ENHANCEMENTS.md
â”‚  â””â”€ Changed Docker files? â†’ Update Docker sections
â”‚
â”œâ”€ Documentation quality checks:
â”‚  â”œâ”€ Are code examples still accurate?
â”‚  â”œâ”€ Do command examples still work?
â”‚  â”œâ”€ Are file references correct?
â”‚  â”œâ”€ Is "Last Updated" date current?
â”‚  â””â”€ Is Changelog updated with changes?
â”‚
â””â”€ Can I use /doc-sync-check skill?
   â””â”€ Automates validation of documentation synchronization
```

---

### ğŸ¯ Phase 9: Pre-Commit Verification

**âš ï¸ NEVER commit without completing this checklist.**

#### If .NET SDK is Available (STANDARD CHECKLIST)

```
PRE-COMMIT CHECKLIST (5 minutes)
â”‚
â”œâ”€ Step 1: Clean build
â”‚  â””â”€ dotnet clean && dotnet restore && dotnet build --no-incremental
â”‚     Expected: 0 errors, 0 warnings
â”‚
â”œâ”€ Step 2: ALL tests pass
â”‚  â””â”€ dotnet test
â”‚     Expected: 568 passing, 14 skipped, 0 failed
â”‚
â”œâ”€ Step 3: Verify new files compile
â”‚  â””â”€ git status (check what's staged)
â”‚
â”œâ”€ Step 4: Check for common issues
â”‚  â”œâ”€ No hardcoded paths (C:\, /Users/, localhost)
â”‚  â”œâ”€ No missing XML documentation warnings
â”‚  â””â”€ Builds in both Debug and Release configurations
â”‚
â”œâ”€ Step 5: Final verification
â”‚  â”œâ”€ git status (review what will be committed)
â”‚  â”œâ”€ git diff --staged (review exact changes)
â”‚  â””â”€ Only THEN commit
â”‚
â””â”€ Step 6: Docker verification (if Dockerfile/docker-compose.yml changed)
   â”œâ”€ docker build -t hotswap-test:local .
   â”œâ”€ docker-compose up -d
   â”œâ”€ curl http://localhost:5000/health
   â””â”€ docker-compose down -v
```

#### If .NET SDK NOT Available (ALTERNATIVE CHECKLIST)

```
ALTERNATIVE CHECKLIST (10 minutes)
â”‚
â”œâ”€ Step 1: Verify contracts before use (READ definitions, don't guess!)
â”‚  â””â”€ For every type used, Read the actual .cs file
â”‚     Check: Property names, method parameters, nullability
â”‚
â”œâ”€ Step 2: Verify package references in .csproj files
â”‚  â””â”€ Test project has packages for types used in test code
â”‚     Example: If using BCrypt.Net.BCrypt, need BCrypt.Net-Next reference
â”‚
â”œâ”€ Step 3: Review all code changes
â”‚  â””â”€ git diff (check for syntax errors, missing using statements)
â”‚
â”œâ”€ Step 4: Verify project references correct
â”‚  â””â”€ Test project references all projects whose types are used
â”‚
â”œâ”€ Step 5: Check for common build errors
â”‚  â”œâ”€ Namespaces match folder structure
â”‚  â”œâ”€ All using statements present
â”‚  â”œâ”€ Async methods return Task
â”‚  â””â”€ Mock setups match actual method signatures
â”‚
â”œâ”€ Step 6: Document CI/CD dependency in commit message
â”‚  â””â”€ Note: "Build/tests will run in GitHub Actions"
â”‚
â””â”€ Step 7: Monitor CI/CD after push
   â””â”€ Check GitHub Actions immediately, fix if fails
```

---

### ğŸ¯ Phase 10: Git Operations

```
GIT WORKFLOW (5 minutes)
â”‚
â”œâ”€ Pre-push: Pull latest changes (MANDATORY)
â”‚  â”œâ”€ git fetch origin <branch-name>
â”‚  â”œâ”€ git pull origin <branch-name> --no-rebase
â”‚  â”œâ”€ If conflicts: Resolve â†’ git add â†’ git commit
â”‚  â”œâ”€ Then rebuild and test: dotnet build && dotnet test
â”‚  â””â”€ Only then push
â”‚
â”œâ”€ Commit message format:
â”‚  â”œâ”€ feat: [description] â†’ New features
â”‚  â”œâ”€ fix: [description] â†’ Bug fixes
â”‚  â”œâ”€ docs: [description] â†’ Documentation changes
â”‚  â”œâ”€ refactor: [description] â†’ Code refactoring
â”‚  â”œâ”€ test: [description] â†’ Test additions/modifications
â”‚  â””â”€ chore: [description] â†’ Maintenance tasks
â”‚
â”œâ”€ Push with retry logic:
â”‚  â”œâ”€ git push -u origin claude/[branch-name]
â”‚  â”œâ”€ Branch MUST start with claude/ and end with session ID
â”‚  â””â”€ On network failure: Retry up to 4 times (2s, 4s, 8s, 16s backoff)
â”‚
â””â”€ Post-push:
   â”œâ”€ Monitor GitHub Actions for build status
   â””â”€ If build fails: Follow emergency fix procedure
```

---

## ğŸ”„ Iteration & Context Management

**For complex investigations spanning multiple cycles:**

### ğŸ”„ Iterative Problem-Solving Framework

**Use OODA Loop: Observe â†’ Orient â†’ Decide â†’ Act**

```
INVESTIGATION CYCLE (Internal - Don't Narrate)
â”‚
â”œâ”€ OBSERVE: Gather specific data for current question
â”‚  â””â”€ Run test, read code, check logs - get ONE piece of info
â”‚
â”œâ”€ ORIENT: Update mental model
â”‚  â””â”€ Does this confirm/refute hypothesis? What's next?
â”‚
â”œâ”€ DECIDE: Choose action
â”‚  â””â”€ Solved? â†’ Implement. Stuck? â†’ Pivot or escape. Progress? â†’ Continue.
â”‚
â””â”€ ACT: Execute
   â””â”€ Write code, run command, read file - DO something
```

**Key Principles:**
- **Don't narrate**: No "Iteration 1, 2, 3..." in output
- **Track internally**: Mental log is fine, written log wastes tokens
- **Deliver results**: Show findings, not investigation process
- **Set limits**: If >10-15 cycles without progress â†’ Use escape sequence

**Balancing Speed vs Thoroughness:**
- **Simple tasks** (80%): Fast path (5 min to first test), minimal narration
- **Complex investigations** (20%): Take time needed (10-30 min OK), but still deliver concise results
- **Rule of thumb**: If investigation takes >30 min, create 50-100 line summary (not 300-line play-by-play)

---

### ğŸ§¹ Context Pruning Strategy

**Manage cognitive load by discarding irrelevant information:**

```
INFORMATION TRIAGE
â”‚
â”œâ”€ ğŸŸ¢ KEEP (Core Facts - Always Relevant)
â”‚  â”œâ”€ Project architecture (layers)
â”‚  â”œâ”€ Current task objective
â”‚  â”œâ”€ Confirmed root cause (if found)
â”‚  â”œâ”€ Active constraints
â”‚  â””â”€ Verified ground truth
â”‚
â”œâ”€ ğŸŸ¡ PARK (Might Be Relevant Later)
â”‚  â”œâ”€ Alternative hypotheses not yet tested
â”‚  â”œâ”€ Tangential issues discovered
â”‚  â”œâ”€ Optimization opportunities
â”‚  â””â”€ Action: Add to TASK_LIST.md or TODO comments
â”‚
â”œâ”€ ğŸ”´ DISCARD (Proven Irrelevant)
â”‚  â”œâ”€ Disproven hypotheses ("Thought it was X, but it's not")
â”‚  â”œâ”€ Dead-end exploration paths
â”‚  â”œâ”€ Red herrings (looked suspicious, actually fine)
â”‚  â””â”€ Action: Explicitly acknowledge "X is NOT the issue"
â”‚
â””â”€ ğŸ”µ SUMMARIZE (Compress for Efficiency)
   â”œâ”€ Long file reads â†’ Extract key facts only
   â”œâ”€ Multiple similar tests â†’ General pattern
   â””â”€ Repeated observations â†’ "Consistently seeing X"
```

**Pruning Triggers (Internal - Don't Output):**
- After finding answer â†’ Discard search details
- When pivoting â†’ Discard failed approach
- When overwhelmed â†’ Mental reset (see below)
- **Don't document pruning process** - just do it

---

### ğŸ“ Mental Reset (When Overwhelmed)

**If losing focus after many search cycles, mentally reset (don't write it out):**

```
INTERNAL CHECKPOINT (Keep in your head, don't output)
â”œâ”€ Goal: What am I solving?
â”œâ”€ Facts: 2-3 confirmed truths
â”œâ”€ Dead ends: What failed?
â”œâ”€ Next: One specific action
â””â”€ Discard rest
```

**Don't create written summaries unless absolutely necessary** - they waste tokens.

---

### ğŸ¯ Breadth-First vs Depth-First Exploration

```
EXPLORATION STRATEGY
â”‚
â”œâ”€ Breadth-First (Survey â†’ Narrow Down)
â”‚  â”‚
â”‚  â”œâ”€ When to use:
â”‚  â”‚  â”œâ”€ Problem area is unfamiliar
â”‚  â”‚  â”œâ”€ Many potential root causes
â”‚  â”‚  â””â”€ Unclear where to start
â”‚  â”‚
â”‚  â””â”€ Approach:
â”‚     1. Quick survey: Grep for keywords
â”‚     2. Skim multiple files (don't deep-dive yet)
â”‚     3. Identify 2-3 most likely areas
â”‚     4. Switch to depth-first on most promising
â”‚
â””â”€ Depth-First (Hypothesis â†’ Verify â†’ Drill Down)
   â”‚
   â”œâ”€ When to use:
   â”‚  â”œâ”€ Have strong hypothesis about root cause
   â”‚  â”œâ”€ Problem is in specific, known area
   â”‚  â””â”€ Following clear chain of causality
   â”‚
   â””â”€ Approach:
      1. Form specific hypothesis
      2. Read relevant code thoroughly
      3. Verify hypothesis with tests/logs
      4. If confirmed â†’ Fix. If not â†’ Backtrack
```

---

### ğŸ”€ When to Pivot vs Persist

```
PIVOT vs PERSIST DECISION
â”‚
â”œâ”€ Persist if:
â”‚  â”œâ”€ Making measurable progress each iteration
â”‚  â”œâ”€ Each iteration narrows down problem space
â”‚  â”œâ”€ Current approach is theoretically sound
â”‚  â””â”€ Time invested is reasonable (<30 min)
â”‚
â”œâ”€ Pivot if:
â”‚  â”œâ”€ Stuck on same question for 3+ iterations
â”‚  â”œâ”€ Observations contradict fundamental assumptions
â”‚  â”œâ”€ Time spent exceeds expected value (>30 min, no progress)
â”‚  â””â”€ Gut feeling says "this doesn't make sense"
â”‚
â””â”€ How to Pivot:
   1. Acknowledge: "Current approach isn't working because..."
   2. Prune: Discard all information specific to failed approach
   3. Keep: Core facts about the problem itself
   4. Rethink: "What if the problem is actually Y, not X?"
   5. New hypothesis: Choose fundamentally different angle
   6. Fresh start: Begin new iteration with new approach
```

---

## ğŸ”“ Deadlock Detection & Escape Sequences

**Preventing and breaking free from stuck states:**

### ğŸš¨ Common Deadlock Patterns

```
DEADLOCK PATTERN CATALOG
â”‚
â”œâ”€ 1. CIRCULAR REASONING LOOP
â”‚  â”‚  Signature: Testing same hypothesis multiple times
â”‚  â”‚  Detection: "Have I already eliminated X? YES â†’ Don't test again"
â”‚  â”‚  Escape: Force completely new angle, or ask for help
â”‚
â”œâ”€ 2. ANALYSIS PARALYSIS
â”‚  â”‚  Signature: Gathering info without deciding (>3 observation iterations)
â”‚  â”‚  Detection: "Have I gathered enough to decide? YES but still not deciding"
â”‚  â”‚  Escape: Timebox 5 minutes, choose "good enough" option, commit
â”‚
â”œâ”€ 3. MISSING INFORMATION DEADLOCK
â”‚  â”‚  Signature: Can't proceed without info X, can't get X myself
â”‚  â”‚  Detection: "Can I get info myself? NO â†’ Missing info deadlock"
â”‚  â”‚  Escape: Ask user for info, or park task and work on something else
â”‚
â”œâ”€ 4. TOOL LIMITATION DEADLOCK
â”‚  â”‚  Signature: Need to do X, but tools can't do X
â”‚  â”‚  Detection: "Trying workarounds that don't quite work"
â”‚  â”‚  Escape: Reframe goal without X, or ask user to do X manually
â”‚
â”œâ”€ 5. COMPLEXITY OVERFLOW DEADLOCK
â”‚  â”‚  Signature: Problem keeps expanding, TodoWrite >10 items
â”‚  â”‚  Detection: "Task scope exceeded original estimate by 3x+"
â”‚  â”‚  Escape: Define MVP, implement minimal version, defer rest
â”‚
â”œâ”€ 6. CONFLICTING CONSTRAINTS DEADLOCK
â”‚  â”‚  Signature: Requirement A says X, Requirement B says NOT X
â”‚  â”‚  Detection: "No solution satisfies both constraints"
â”‚  â”‚  Escape: Ask user which takes priority
â”‚
â””â”€ 7. FALSE ASSUMPTION DEADLOCK
   â”‚  Signature: All approaches fail, reality doesn't match expectations
   â”‚  Detection: "This SHOULD work but doesn't" (3+ iterations)
   â”‚  Escape: List all assumptions, verify each, rebuild mental model
```

---

### ğŸ”“ Universal Escape Sequences

**These work for ANY deadlock:**

```
UNIVERSAL ESCAPE PROTOCOL
â”‚
â”œâ”€ ESCAPE #1: THE RESET
â”‚  â”‚  When: Completely stuck, nothing working
â”‚  â”‚  Steps:
â”‚  â”‚  1. STOP ALL WORK
â”‚  â”‚  2. SAVE STATE: Create Working Memory Summary
â”‚  â”‚  3. CLEAR MIND: Discard speculation, keep only facts
â”‚  â”‚  4. BREAK: Move to different task
â”‚  â”‚  5. RETURN FRESH: Begin from Phase 1 as if new task
â”‚
â”œâ”€ ESCAPE #2: ASK FOR HELP
â”‚  â”‚  When: Stuck after trying reset, or missing critical info
â”‚  â”‚  Template:
â”‚  â”‚     "I'm stuck on [problem]. Tried:
â”‚  â”‚      - [Approach A] â†’ [Result]
â”‚  â”‚      - [Approach B] â†’ [Result]
â”‚  â”‚
â”‚  â”‚      Learned: [Facts]
â”‚  â”‚      Stuck on: [Specific blocker]
â”‚  â”‚
â”‚  â”‚      Options:
â”‚  â”‚      1. [Option A - pros/cons]
â”‚  â”‚      2. [Option B - pros/cons]
â”‚  â”‚
â”‚  â”‚      What would you recommend?"
â”‚
â”œâ”€ ESCAPE #3: SIMPLIFY RADICALLY
â”‚  â”‚  When: Problem seems too complex
â”‚  â”‚  Steps:
â”‚  â”‚  1. IDENTIFY CORE: What's the ONE thing I'm really trying to do?
â”‚  â”‚  2. STRIP AWAY: Remove all nice-to-haves
â”‚  â”‚  3. ABSOLUTE MINIMUM: Simplest version that could work?
â”‚  â”‚  4. IMPLEMENT THAT: Just the core
â”‚  â”‚  5. TEST: Does minimal version work?
â”‚
â”œâ”€ ESCAPE #4: CHANGE PERSPECTIVE
â”‚  â”‚  When: Stuck in one way of thinking
â”‚  â”‚  Perspectives:
â”‚  â”‚  - User perspective: "What does user experience?"
â”‚  â”‚  - Data perspective: "What happens to the data?"
â”‚  â”‚  - Timeline perspective: "What happens in what order?"
â”‚  â”‚  - Reverse perspective: "What if I start from the end?"
â”‚
â””â”€ ESCAPE #5: DIVIDE AND CONQUER
   â”‚  When: Problem has multiple interacting parts
   â”‚  Steps:
   â”‚  1. IDENTIFY COMPONENTS: What are the moving parts?
   â”‚  2. ISOLATE: Test each component independently
   â”‚  3. VERIFY: Which work? Which don't?
   â”‚  4. NARROW: Focus only on failing component
```

---

### â±ï¸ Deadlock Detection Checklist

**Run this every 3-5 iterations:**

```
DEADLOCK SELF-CHECK
â”‚
Ask yourself:
â”‚
â”œâ”€ 1. Am I making progress?
â”‚  â””â”€ Each iteration should teach something new
â”‚     If last 3 iterations taught nothing â†’ DEADLOCK
â”‚
â”œâ”€ 2. Am I repeating myself?
â”‚  â””â”€ Have I tested this hypothesis before?
â”‚     If YES â†’ CIRCULAR REASONING
â”‚
â”œâ”€ 3. Can I make a decision?
â”‚  â””â”€ Do I have enough info to choose?
â”‚     If YES but still gathering â†’ ANALYSIS PARALYSIS
â”‚
â”œâ”€ 4. Do I have what I need?
â”‚  â””â”€ Is there critical info I can't access?
â”‚     If YES â†’ MISSING INFORMATION
â”‚
â”œâ”€ 5. Is scope expanding?
â”‚  â””â”€ Is problem bigger than when I started?
â”‚     If 3x+ bigger â†’ COMPLEXITY OVERFLOW
â”‚
â”œâ”€ 6. Are requirements consistent?
â”‚  â””â”€ Can all constraints be satisfied?
â”‚     If NO â†’ CONFLICTING CONSTRAINTS
â”‚
â”œâ”€ 7. Does reality match expectations?
â”‚  â””â”€ Are observations matching predictions?
â”‚     If consistently NO â†’ FALSE ASSUMPTION
â”‚
â””â”€ 8. Am I fighting my tools?
   â””â”€ Trying to do something tools can't do?
      If YES â†’ TOOL LIMITATION

IF ANY DEADLOCK DETECTED:
â””â”€ STOP immediately
â””â”€ Identify pattern (1-7 above)
â””â”€ Execute pattern-specific escape
â””â”€ If still stuck â†’ Execute universal escape
```

---

### ğŸ“Š Deadlock Prevention Strategies

```
PREVENTION CHECKLIST (Run at start of complex task)
â”‚
â”œâ”€ Before Starting:
â”‚  â”œâ”€ [ ] Define success criteria (what does "done" look like?)
â”‚  â”œâ”€ [ ] Set iteration limit (max 10 before reset)
â”‚  â”œâ”€ [ ] Identify required information
â”‚  â”œâ”€ [ ] Define MVP scope
â”‚  â”œâ”€ [ ] Check for conflicting constraints
â”‚  â”œâ”€ [ ] List assumptions explicitly
â”‚  â”œâ”€ [ ] Plan escape route (if stuck, I'll do X)
â”‚  â””â”€ [ ] Set time limit (max 1 hour before checkpoint)
â”‚
â”œâ”€ During Work:
â”‚  â”œâ”€ [ ] Track eliminated hypotheses
â”‚  â”œâ”€ [ ] Update iteration log
â”‚  â”œâ”€ [ ] Prune context regularly
â”‚  â”œâ”€ [ ] Question assumptions periodically
â”‚  â”œâ”€ [ ] Check progress every 3 iterations
â”‚  â””â”€ [ ] Use TodoWrite for complex tasks
â”‚
â””â”€ Early Warning Signs:
   â”œâ”€ âš ï¸ Revisiting same file 3+ times
   â”œâ”€ âš ï¸ Reading more than coding (10:1 ratio)
   â”œâ”€ âš ï¸ "Just one more check" thoughts
   â”œâ”€ âš ï¸ Uncertainty increasing instead of decreasing
   â””â”€ âš ï¸ Time spent exceeds estimate by 2x
      ACTION: Run deadlock self-check immediately
```

---

## ğŸ§  Advanced Topics

### ğŸ§ª Minimal Experiment Design

**When exploring unknowns:**

```
EXPERIMENT PRINCIPLES
â”‚
Each experiment should:
â”œâ”€ Test ONE specific hypothesis
â”œâ”€ Be reversible (easy to undo)
â”œâ”€ Take <5 minutes to set up and run
â”œâ”€ Produce clear pass/fail result
â””â”€ Teach you something regardless of outcome

Good example:
  Hypothesis: "Method X is never called during test Y"
  Experiment: Add Console.WriteLine("X called") at start of X
  Run: dotnet test --filter Y
  Observe: If no output â†’ Confirmed. If output â†’ Refuted.
  Cleanup: Remove Console.WriteLine
```

---

### ğŸ“Š Investigation Documentation (When Needed)

**ONLY create investigation docs for:**
- User-requested investigations ("why are X tests failing?")
- Findings that affect future work (documented decisions)
- Complex root cause analysis that should be referenced later

**DON'T create docs for:**
- Routine debugging (just fix it)
- Simple searches (just deliver answer)
- Code exploration (just write the code)

**If documenting, keep it CONCISE (<100 lines):**

```markdown
# [Problem] Investigation

**Root Cause**: [One sentence]
**Found**: [2-3 key discoveries]
**Decision**: [What action was taken]
**Impact**: [Risk level + justification]

## Recommendations
1. [Action A] - [Effort estimate]
2. [Action B] - [Effort estimate]

## Tasks Created
- Task #N: [Description] ([Effort])
```

**Focus on findings and decisions, NOT investigation process. No "Iteration" logs.**

---

## ğŸš¨ Critical Reminders

**These are NON-NEGOTIABLE:**

```
âŒ NEVER commit without running pre-commit checklist
âŒ NEVER guess property/method names (always Read definitions)
âŒ NEVER skip writing tests first (TDD is mandatory)
âŒ NEVER commit with failing tests
âŒ NEVER push without pulling first
âŒ NEVER ignore build warnings
âŒ NEVER skip documentation updates
âŒ NEVER commit secrets or environment-specific values

âœ… ALWAYS follow Red-Green-Refactor cycle
âœ… ALWAYS verify contracts before use (read definitions)
âœ… ALWAYS run dotnet build && dotnet test before commit
âœ… ALWAYS update documentation with code changes
âœ… ALWAYS pull before push
âœ… ALWAYS use proper git branch naming (claude/*-sessionid)
âœ… ALWAYS monitor CI/CD after pushing
âœ… ALWAYS check package references in test projects
```

---

## ğŸ’¡ Quick Decision Tree

```
START
  â”‚
  â”œâ”€ Is request clear? NO â†’ Ask clarifying questions
  â”‚                    YES â†“
  â”œâ”€ Do I have .NET SDK? NO â†’ Use alternative checklist
  â”‚                      YES â†“
  â”œâ”€ Have I read relevant contracts? NO â†’ Use Read tool
  â”‚                                  YES â†“
  â”œâ”€ Have I written tests first? NO â†’ Write failing tests (RED)
  â”‚                              YES â†“
  â”œâ”€ Do tests pass? NO â†’ Implement code (GREEN)
  â”‚                 YES â†“
  â”œâ”€ Is code quality good? NO â†’ Refactor (BLUE)
  â”‚                        YES â†“
  â”œâ”€ Are docs updated? NO â†’ Update documentation
  â”‚                    YES â†“
  â”œâ”€ Does build pass? NO â†’ Fix errors, don't commit
  â”‚                   YES â†“
  â”œâ”€ Do ALL tests pass? NO â†’ Fix failures, don't commit
  â”‚                     YES â†“
  â”œâ”€ Have I pulled latest? NO â†’ git pull origin <branch>
  â”‚                        YES â†“
  â”œâ”€ Am I ready to commit? YES â†’ Commit with proper message
  â”‚
  â””â”€ Push with retry logic â†’ Monitor CI/CD â†’ DONE âœ…
```

---

**Remember**: This is a high-quality, production-ready codebase. Maintain that standard in every contribution! ğŸ¯

**When in doubt**: Bias toward ACTION. Write code, iterate quickly, course-correct as you go.
